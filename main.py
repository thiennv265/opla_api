import subprocess, io, sys, time, traceback

def install_if_missing(package):
  try:
    __import__(package)
  except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--break-system-packages"])
# C·∫ßn ki·ªÉm tra v√† c√†i c√°c th∆∞ vi·ªán ngo√†i
for pkg in ["requests", "pandas", "fastapi", "cachetools", "urllib3", "openpyxl", "numpy", "simplejson", "openpyxl", "rapidfuzz", "tqdm"]:
  install_if_missing(pkg)
from typing import List
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import openpyxl
from rapidfuzz import fuzz, process
import requests
import json
import numpy as np
import pandas as pd
pd.set_option('future.no_silent_downcasting', True)
from fastapi import FastAPI, Query, Response, Request, HTTPException
from datetime import datetime, timezone, timedelta
from threading import Lock
from fastapi.responses import JSONResponse
import simplejson as json
import logging
import re

from starlette.exceptions import HTTPException as StarletteHTTPException
telegram_token = "7069011696:AAHTEO8CmfHKebxAh8TBjMb73wKZt6nbDFg"
app = FastAPI(docs_url = "/docs/guide", redoc_url = None, openapi_url="/openapi.json")
cache = {}
lock = Lock()

@app.exception_handler(StarletteHTTPException)
async def custom_404_handler(request, exc):
    if exc.status_code == 404:
        return JSONResponse(status_code=404, content={"Here the error":"Stupid! 404"})
    return JSONResponse(status_code=exc.status_code, content={"error": exc.detail})

@app.exception_handler(Exception)
async def unhandled_exception_handler(request, exc):
    # C√≥ th·ªÉ log traceback t·∫°i ƒë√¢y
    return JSONResponse(
        status_code=500,
        content={"Here the error": "Internal server error 500"}
    )
    
from fastapi.exceptions import RequestValidationError
from starlette.responses import JSONResponse

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    return JSONResponse(
        status_code=422,
        content={"Here the error": "Double check. Please!"}
    )
    
# L·∫•y gi·ªù GMT+7
def get_current_time_str():
  tz_gmt7 = timezone(timedelta(hours=7))
  now = datetime.now(tz_gmt7)
  return now.strftime('%Y-%m-%d_%H-%M-%S')

def send_excel_to_telegram(file_bytes: bytes, filename: str, chat_id: str, bot_token: str):
    files = {
        'document': (filename, file_bytes, 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet')
    }
    data = {
        'chat_id': chat_id,
        'caption': f'üì¶ File: {filename}'
    }

    url = f"https://api.telegram.org/bot{bot_token}/sendDocument"
    response = requests.post(url, data=data, files=files)
    return response

def send_log(msg, sender_name=None, telegram_token = telegram_token):
    if telegram_token:
      try:
          requests.get(
              f"https://api.telegram.org/bot{telegram_token}/sendMessage",
              params={"chat_id": "716085753", "text": "üÖæÔ∏è" + get_current_time_str() + " " + f"{sender_name}\n{msg}" if sender_name else "\n" + msg,"disable_web_page_preview": True}
          )
      except Exception as e:
          print(e)
    else:
      print("No TELE TOKEN ")
      
def convert_utc_to_gmt7(dt_str: str) -> str:
    """
    Chuy·ªÉn chu·ªói th·ªùi gian ISO UTC (c√≥ ƒë·ªãnh d·∫°ng: 2025-06-26T05:09:58.660403+00:00)
    sang ƒë·ªãnh d·∫°ng t∆∞∆°ng ·ª©ng trong GMT+7.

    Tr·∫£ v·ªÅ chu·ªói ISO format theo GMT+7.
    """
    try:
        # Parse t·ª´ chu·ªói c√≥ timezone UTC
        dt_utc = datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f%z")
        # Chuy·ªÉn sang GMT+7
        gmt7 = dt_utc.astimezone(timezone(timedelta(hours=7)))
        return str(gmt7)
    except Exception as e:
        print(e)
        raise ValueError(f"Kh√¥ng th·ªÉ parse th·ªùi gian: {e}")
      
class CustomFormatter(logging.Formatter):
    YELLOW = "\033[93m"
    RESET = "\033[0m"
    def formatTime(self, record, datefmt=None):
        return f"{self.YELLOW}{get_current_time_str()}{self.RESET}"

# Logger ri√™ng (ho·∫∑c d√πng uvicorn.access n·∫øu mu·ªën)
logger = logging.getLogger("masked.access")
logger.setLevel(logging.INFO)
handler = logging.StreamHandler()
formatter = CustomFormatter('%(asctime)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def mask_query_params(url: str, keys=("token", "secrets")) -> str:
    for key in keys:
        pattern = rf"({key}=)([^&\s]+)"
        url = re.sub(pattern, lambda m: m.group(1) + mask_half(m.group(2)), url)
    return url

def mask_half(value: str) -> str:
    if len(value) < 4:
        return "***"
    half = len(value) // 2
    return value[:half] + "*" * (len(value) - half)

def json_size(data):
    json_bytes = json.dumps(data).encode("utf-8")
    size_mb = len(json_bytes) / (1024 * 1024)
    return f"{size_mb:.1f} MB"

def color_status(status_code: int) -> str:
    if 200 <= status_code < 300:
        return f"\033[92m{status_code}\033[0m"  # xanh l√°
    elif 300 <= status_code < 400:
        return f"\033[96m{status_code}\033[0m"  # cyan
    elif 400 <= status_code < 500:
        return f"\033[93m{status_code}\033[0m"  # v√†ng
    else:
        return f"\033[91m{status_code}\033[0m"  # ƒë·ªè

@app.middleware("http")
async def log_masked_requests(request: Request, call_next):
    start = time.time()
    # client_ip = request.client.host
    client_ip = request.headers.get("X-Forwarded-For", request.client.host).split(",")[0].strip()
    method = request.method
    url = str(request.url)
    masked_url = mask_query_params(url)
    response = await call_next(request)
    status_code = response.status_code
    duration_ms = time.time() - start
    msg_print = f"\033[91m{client_ip}\033[0m - {method} {masked_url} - {color_status(status_code)} - {duration_ms:.2f}s"
    logger.info(msg_print)
    msg_sendlog = f"{client_ip} - {method} {masked_url} - {status_code} - {duration_ms:.2f}s"
    send_log(msg_sendlog,"main")
    return response

def dedup_dicts_smart(data: list[dict]) -> list[dict]:
    try:
        # return pd.DataFrame(data).drop_duplicates().to_dict(orient="records")
        df = pd.DataFrame(data).drop_duplicates()
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df = df.where(pd.notnull(df), None)
        return df
    except Exception as e:
        print(e)
        # return dedup_large_dict_list(data)
      
def appendToRow(myDict: dict, key: str, value: str):
  myDict[key] = value
    
def getdata(token: str):
    try:
        sta = get_current_time_str()
        raw_rows = []
        raw_logs = []
        total_bytes = 0
        for skipp in range(0,30001,150):
            url = f"https://api-admin.oplacrm.com/api/public/opportunities?take=170&skip={skipp - 20 if skipp > 0 else 0}"
            # url = f"https://api-admin.oplacrm.com/api/public/opportunities?take=10"
            headers = {"Authorization": token}
            response = requests.get(url, headers=headers, verify = False)
            if response.status_code == 200:
                sources = response.json()
                size = len(json.dumps(sources).encode('utf-8'))  # t√≠nh size theo byte
                total_bytes += size
                excluded_keys = ["weight","area","google_map_address","description","stage_compact","amount", "invoice", "invoices", "opportunity_process",
                               "opportunity_process_stage_id","tax_inclusive_amount","forecast","opportunities_joint","opportunities_products","date_closed",
                               "locked","date_closed_actual","discussions","is_parent","source","opportunity_status","project_type","opportunities_contacts",
                               "Error","notes","parent_opportunity_id","parent_opportunity","opportunities_children","opportunity_type_id","activities","date_open"]
                special_keys = ["custom_field_opportunity_values","opportunity_process_stage","owner","users_opportunities","accounts_opportunities","created_at","stage_logs"]
                for index, item in enumerate(sources):
                    row = {}
                    for key, value in item.items():
                      # print(key)
                      if key not in excluded_keys:
                        if key not in special_keys:
                          appendToRow(row, f'store_{key}',value)
                        elif key == "created_at":
                          appendToRow(row, f'store_{key}',value[:10])
                        elif key == "stage_logs":
                          # print(len(value))
                          for i in value:
                            row_log = {}
                            appendToRow(row_log, f'store_id',item["id"])
                            appendToRow(row_log, f'store_short_id',item["short_id"])
                            appendToRow(row_log, f'creator',i["creator"]["email"])
                            appendToRow(row_log, f'datetime', convert_utc_to_gmt7(i["created_at"]))
                            appendToRow(row_log, f'stage',i["new_stage"])
                            raw_logs.append(row_log)
                        elif key =="custom_field_opportunity_values":
                          for i in value:
                            print(i["custom_field"])
                            if i["custom_field"]["name"] not in ["20. ADO","23. Gi√° m√≥n trung b√¨nh *","18. Vƒ© ƒë·ªô","19. Kinh ƒë·ªô","21. Qu·∫≠n *","Qu·∫≠n (c≈©)",
                                                                 "24. Ph·∫ßn m·ªÅm b√°n h√†ng *","23. Khung gi·ªù ho·∫°t ƒë·ªông 2","25. Ghi Ch√∫ Ri√™ng"]:
                                appendToRow(row, f'store_{i["custom_field"]["name"]}',i["value"])
                        elif key == "opportunity_process_stage":
                          appendToRow(row, f'store_{key}',value["opportunity_stage"]["name"])
                        elif key == "owner":
                          appendToRow(row, f'store_{key}',value["email"])
                        elif key == "users_opportunities":
                          appendToRow(row, f'store_{key}',value[0]["user"]["email"])
                        elif key == "accounts_opportunities":
                          for k, v in value[0]["account"].items():
                            included_keys = ["id","name","short_id","account_type","owner","custom_field_account_values","tax_identification_number"]
                            if k in included_keys:
                              if k == "account_type":
                                appendToRow(row, f'mex_{k}',v)
                              elif k == "tax_identification_number":
                                appendToRow(row, f'mex_tax_id',v)
                              elif k == "owner":
                                appendToRow(row, f'mex_{k}',v["email"])
                              elif k == "custom_field_account_values":
                                for k1 in v:
                                  excluded_keys_mex = ["34. Link ·∫£nh","21. Ph·∫ßn m·ªÅm b√°n h√†ng *","28. Ghi ch√∫ tr·∫°ng th√°i","27. Tr·∫°ng th√°i k√Ω k·∫øt *","29. L√Ω do Kh√¥ng H·ª£p L·ªá",
                                                       "24. Ph·∫ßn m·ªÅm b√°n h√†ng *","23. Khung gi·ªù ho·∫°t ƒë·ªông 2","25. Ghi Ch√∫ Ri√™ng","20. ADO","23. Gi√° m√≥n trung b√¨nh *","24. Link gian h√†ng SPF",
                                                      "25. Link gian h√†ng GF", "27. Link gian h√†ng Google Review","33. Link gian h√†ng BeF","account_type"]
                                  n = k1["custom_field"]["name"]
                                  vl = k1["value"]
                                  if k1["custom_field"]["master_data_custom_fields"]:
                                    for i in k1["custom_field"]["master_data_custom_fields"]:
                                      if i["id"] == vl:
                                        vl = i["value"]
                                  if n not in excluded_keys_mex:
                                    appendToRow(row, f'mex_{n}',vl)
                              else:
                                appendToRow(row, f'mex_{k}',v)
                        else:
                            appendToRow(row, f'store_{key}',value)
                    raw_rows.append(row)
            else:
              return f'Error: {response.status_code} {response.text}'
            if len(sources) < 170: break
        store_records = dedup_dicts_smart(raw_rows)
        store_logs = dedup_dicts_smart(raw_logs)
        sto = get_current_time_str()
        msgg = f"   {sta} -> {sto}: {total_bytes / (1024 * 1024):.2f} MB - {len(store_records)} store records + {len(store_logs)} log records"
        print (msgg)
        send_log(msgg,"main")
        return [store_records, store_logs]
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

def getleads(token: str):
    try:
        sta = get_current_time_str()
        raw_rows = []
        total_bytes = 0
        url = f"https://api-admin.oplacrm.com/api/public/leads?take=100000"
        headers = {
            "Authorization": token
        }
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            sources = response.json()
            size = len(json.dumps(sources).encode('utf-8'))  # t√≠nh size theo byte
            total_bytes += size
            included_keys = ["account_name","name","id","created_at","custom_field_lead_values","owner","phone"]
            for index, item in enumerate(sources):
                row = {}
                for key, value in item.items():
                    if key =="custom_field_lead_values":
                        for i in value:
                            appendToRow(row, f'{i["custom_field"]["name"]}',i["value"])
                    elif key == "account_name":
                        # print(value)
                        appendToRow(row, 'store_lead',value)
                    elif key == "name":
                        appendToRow(row, 'contact_name',value)
                    elif key == "phone":
                        appendToRow(row, 'phone',f'0{value["phone"]}')
                    elif key == "id":
                        appendToRow(row, 'lead_id',value)
                    elif key == "created_at":
                        appendToRow(row, 'created_at',value[:10])
                    elif key == "owner":
                        appendToRow(row, 'owner',value['full_name'])
                        appendToRow(row, 'owner_id',value['external_id'])
                raw_rows.append(row)
        else:
            return f'Error: {response.status_code} {response.text}'
        dunique = dedup_dicts_smart(raw_rows)
        sto = get_current_time_str()
        msgg = f"   {sta} -> {sto}: {total_bytes / (1024 * 1024):.2f} MB - {len(dunique)} lead records"
        # print (msgg)
        send_log(msgg,"main")
        return dunique
    except Exception as e:
        print(traceback.print_exc())
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

def convert_all_columns_to_str(df: pd.DataFrame) -> pd.DataFrame:
    """
    √âp to√†n b·ªô c√°c c·ªôt trong DataFrame v·ªÅ ki·ªÉu str, x·ª≠ l√Ω c·∫£ NaN.
    """
    return df.applymap(lambda x: '' if pd.isna(x) else str(x))
    
def processing_logs(logs_df, current_df):
    try:
        # X·ª≠ l√Ω datetime v√† chu·∫©n h√≥a stage
        logs_df['datetime'] = pd.to_datetime(logs_df['datetime'])
        logs_df['stage'] = logs_df['stage'].str.strip()

        # L·∫•y ng√†y Ch·ªù duy·ªát (m·ªõi nh·∫•t)
        cho_duyet = (
            logs_df[logs_df['stage'] == 'Ch·ªù Duy·ªát']
            .drop_duplicates('store_id', keep='first')[['store_id', 'datetime']]
            .rename(columns={'datetime': 'logs Ng√†y Ch·ªù duy·ªát'})
        )
        cho_duyet['logs Ng√†y Ch·ªù duy·ªát'] = cho_duyet['logs Ng√†y Ch·ªù duy·ªát'].dt.strftime('%Y-%m-%d')  # -> chu·ªói

        # L·∫•y ng√†y Ph√™ duy·ªát (m·ªõi nh·∫•t t·ª´ "C·∫ßn ƒëi·ªÅu ch·ªânh", "ƒê·ªß th√¥ng tin")
        phe_duyet = (
            logs_df[logs_df['stage'].isin(['C·∫ßn ƒëi·ªÅu ch·ªânh', 'ƒê·ªß th√¥ng tin'])]
            .drop_duplicates('store_id', keep='first')[['store_id', 'datetime']]
            .rename(columns={'datetime': 'logs Ng√†y Ph√™ duy·ªát'})
        )
        phe_duyet['logs Ng√†y Ph√™ duy·ªát'] = phe_duyet['logs Ng√†y Ph√™ duy·ªát'].dt.strftime('%Y-%m-%d')  # -> chu·ªói

        # G·ªôp logs l·∫°i
        logs_summary = cho_duyet.merge(phe_duyet, on='store_id', how='outer')

        # Chuy·ªÉn ng√†y trong current v·ªÅ d·∫°ng chu·ªói (ƒë·ªÉ so s√°nh v√† export)
        current_df['store_Ng√†y Ch·ªù duy·ªát'] = pd.to_datetime(current_df['store_Ng√†y Ch·ªù duy·ªát']).dt.strftime('%Y-%m-%d')
        current_df['store_Ng√†y Ph√™ duy·ªát'] = pd.to_datetime(current_df['store_Ng√†y Ph√™ duy·ªát']).dt.strftime('%Y-%m-%d')

        # Ch·ªçn c√°c c·ªôt c·∫ßn t·ª´ current v√† g·ªôp v·ªõi logs
        merged = current_df[['store_id', 'store_short_id', 'store_Ng√†y Ch·ªù duy·ªát', 'store_Ng√†y Ph√™ duy·ªát']].merge(
            logs_summary, on='store_id', how='left'
        )

        # So s√°nh ng√†y
        merged['Check Ch·ªù Duy·ªát'] = merged['store_Ng√†y Ch·ªù duy·ªát'] == merged['logs Ng√†y Ch·ªù duy·ªát']
        merged['Check Ph√™ Duy·ªát'] = merged['store_Ng√†y Ph√™ duy·ªát'] == merged['logs Ng√†y Ph√™ duy·ªát']

        # T·∫°o c·ªôt correct n·∫øu l·ªách
        merged['correct Ng√†y Ch·ªù duy·ªát'] = merged.apply(
            lambda row: row['logs Ng√†y Ch·ªù duy·ªát'] if pd.notna(row['logs Ng√†y Ch·ªù duy·ªát']) and row['logs Ng√†y Ch·ªù duy·ªát'] != row['store_Ng√†y Ch·ªù duy·ªát'] else '',
            axis=1
        )
        merged['correct Ng√†y Ph√™ duy·ªát'] = merged.apply(
            lambda row: row['logs Ng√†y Ph√™ duy·ªát'] if pd.notna(row['logs Ng√†y Ph√™ duy·ªát']) and row['logs Ng√†y Ph√™ duy·ªát'] != row['store_Ng√†y Ph√™ duy·ªát'] else '',
            axis=1
        )

        # K·∫øt qu·∫£ cu·ªëi c√πng
        final_result = merged[[
            'store_id', 'store_short_id',
            'logs Ng√†y Ch·ªù duy·ªát', 'store_Ng√†y Ch·ªù duy·ªát', 'Check Ch·ªù Duy·ªát', 'correct Ng√†y Ch·ªù duy·ªát',
            'logs Ng√†y Ph√™ duy·ªát', 'store_Ng√†y Ph√™ duy·ªát', 'Check Ph√™ Duy·ªát', 'correct Ng√†y Ph√™ duy·ªát'
        ]]
        return final_result
        # # Xu·∫•t file Excel
        # final_result.to_excel("ket_qua_so_sanh.xlsx", index=False)

        # # In th·ª≠ k·∫øt qu·∫£
        # print(final_result)
    except Exception as e:
        traceback.print_exc()
        send_log(f"L·ªói {e}", "main")
        return None
@app.get("/")
def home():
  return {"hello":":)"}

@app.get("/opla/")
def api_opla(
    token: str = Query(...),
    secrets: str = Query(...),
    fields: List[str] = Query(None),
    limit: int = Query(None),
    export: int = Query(None)
):
    try:
        if secrets == 'chucm@ym@n8686':
            if cache.get(token, {}).get("stores") is None:
                data_stores = getdata(token)
                with lock:
                    if len(data_stores[0]) > 0:
                      if token not in cache: cache[token] = {}
                      cache[token]["stores"] = data_stores[0]
                      cache[token]["stage_logs"] = data_stores[1]
                      cache[token]["updated_stores_and_stage_logs"] = get_current_time_str()
            df = pd.DataFrame(cache[token]["stores"])
            if limit:
                df = df.iloc[:limit]
            if fields:
                valid_fields = [col for col in fields if col in df.columns]
                df = df[valid_fields]
            if not export or export != 1:
                safe_json = json.dumps(
                    df.to_dict(orient="records"),
                    ignore_nan=True
                )
                return Response(content=safe_json, media_type="application/json")
            elif export == 1:                   
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine="openpyxl") as writer:
                    df.to_excel(writer, index=False, sheet_name=cache[token]["updated_stores_and_stage_logs"])

                output.seek(0)
                file_content = output.read()
                
                file_bytes = io.BytesIO(file_content)
                file_bytes.seek(0)
                # G·ª≠i t·ªõi Telegram
                send_excel_to_telegram(
                    file_bytes= file_bytes,
                    filename=f"api_store_{cache[token]['updated_stores_and_stage_logs']}.xlsx",
                    chat_id="716085753",
                    bot_token=telegram_token
                )

                # Tr·∫£ response
                headers = {
                    "Content-Disposition": f"attachment; filename=api_store_{cache[token]['updated_stores_and_stage_logs']}.xlsx"
                }
                return Response(
                    content=file_content,
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers=headers
                )
        else:
            return {"L·ªói": "Sai secrets :("}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/logs/")
def api_logs(token: str = Query(...),secrets: str = Query(...),fields: List[str] = Query(None),limit: int = Query(None),export: int = Query(None)):
    try:
        if secrets == 'chucm@ym@n8686':
            if cache.get(token, {}).get("stage_logs") is None:
                data_stores = getdata(token)
                with lock:
                    if len(data_stores[1]) > 0:
                      if token not in cache: cache[token] = {}
                      cache[token]["stores"] = data_stores[0]
                      cache[token]["stage_logs"] = data_stores[1]
                      cache[token]["updated_stores_and_stage_logs"] = get_current_time_str()

            df = pd.DataFrame(cache[token]["stage_logs"])
            df_current = pd.DataFrame(cache[token]["stores"])
            expected_cols = ["store_id","store_short_id","store_Ng√†y Ch·ªù duy·ªát","store_Ng√†y Ph√™ duy·ªát"]
            available_cols = [col for col in expected_cols if col in df_current.columns]
            df_current = df_current[available_cols]
            df_processing = processing_logs(df,df_current)
            # print(df)
            # print(df_current)
            if limit:
                df = df.iloc[:limit]
            if fields:
                valid_fields = [col for col in fields if col in df.columns]
                df = df[valid_fields]

            if not export or export != 1:
                return {"Th√¥ng b√°o":"Kh√¥ng xem ƒë∆∞·ª£c :)"}
            elif export == 1:                   
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine="openpyxl") as writer:
                    convert_all_columns_to_str(df).to_excel(writer, index=False, sheet_name="logs")
                    convert_all_columns_to_str(df_current).to_excel(writer, index=False, sheet_name="current")
                    if df_processing:
                        convert_all_columns_to_str(df_processing).to_excel(writer, index=False, sheet_name="processing")

                output.seek(0)
                file_content = output.read()
                file_bytes = io.BytesIO(file_content)
                file_bytes.seek(0)
                # G·ª≠i t·ªõi Telegram
                send_excel_to_telegram(
                    file_bytes= file_bytes,
                    filename=f"api_logs_{cache[token]['updated_stores_and_stage_logs']}.xlsx",
                    chat_id="716085753",
                    bot_token=telegram_token
                )

                # Tr·∫£ response
                headers = {
                    "Content-Disposition": f"attachment; filename=api_logs_{cache[token]['updated_stores_and_stage_logs']}.xlsx"
                }
                return Response(
                    content=file_content,
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers=headers
                )
        return {"L·ªói": "Sai secrets :("}
    except Exception as e:
      print(traceback.print_exc())
      raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")
        
@app.get("/leads/")
def api_lead(
    token: str = Query(...),
    secrets: str = Query(...),
    fields: List[str] = Query(None),
    limit: int = Query(None),
    export: int = Query(None)
):
    try:
        if secrets != 'chucm@ym@n8686': return {}
        if cache.get(token, {}).get("leads") is None:
            data_leads = getleads(token)
            with lock:
                if len(data_leads) > 0:
                  if token not in cache: cache[token] = {}
                  cache[token]["leads"] = data_leads,
                  cache[token]["updated_leads"] = get_current_time_str()
        df = pd.DataFrame(cache[token]["leads"])
        if fields: df = df[[col for col in fields if col in df.columns]]
        if limit: df = df.iloc[:limit]
        if not export or export != 1:
            return Response(
                content=json.dumps(df.to_dict(orient="records"), ignore_nan=True),
                media_type="application/json"
            )
        elif export == 1:
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                df.to_excel(writer, index=False, sheet_name=cache[token]["updated_leads"])

            output.seek(0)
            file_content = output.read()

            file_bytes = io.BytesIO(file_content)
            file_bytes.seek(0)
            # G·ª≠i t·ªõi Telegram
            send_excel_to_telegram(
                file_bytes= file_bytes,
                filename=f"api_leads_{cache[token]['updated_leads']}.xlsx",
                chat_id="716085753",
                bot_token=telegram_token
            )

            # Tr·∫£ response
            headers = {
                "Content-Disposition": f"attachment; filename=api_leads_{cache[token]['updated_leads']}.xlsx"
            }
            return Response(
                content=file_content,
                media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                headers=headers
            )

    except Exception as e:
        print(traceback.print_exc())
        raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

@app.get("/f5/")
def api_f5(token: str = Query(...), secrets: str = Query(...)):
  if secrets == 'chucm@ym@n8686':
    if cache.get(token, {}).get("data") is not None:
        del cache[token]
    data_stores = getdata(token)
    with lock:
        if len(data_stores[0]) > 0:
          if token not in cache: cache[token] = {}
          cache[token]["stores"] = data_stores[0]
          cache[token]["stage_logs"] = data_stores[1]
          cache[token]["updated_stores_and_stage_logs"] = get_current_time_str()
    send_log("Refreshed!", "main")
    return {'result': 'OK :)'}
  else:
    return {"L·ªói": "Sai secrets :("}
    
@app.get("/f5/clear-all")
def api_clear(secrets: str = Query(...)):
  if secrets == 'chucm@ym@n8686':
    cache = {}
    send_log("Empty!", "main")
    return {'result': 'OK :)'}
  else:
    return {"L·ªói": "Sai secrets :("}

@app.get("/updated/")
def last_update(token: str = Query(...)):
  try:
    return {
    "updated_stores_and_stage_logs": cache.get(token, {}).get("updated_stores_and_stage_logs"),
    "updated_leads": cache.get(token, {}).get("updated_leads")}
  except Exception as e:
    raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")
