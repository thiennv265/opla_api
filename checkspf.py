import streamlit as st
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import pandas as pd
from io import BytesIO
import openpyxl
from openpyxl.utils.dataframe import dataframe_to_rows
import pandas as pd
from selenium.webdriver.common.by import By
# N·∫øu file ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
import os

from datetime import datetime

def current_timestamp():
    """
    Tr·∫£ v·ªÅ chu·ªói th·ªùi gian hi·ªán t·∫°i d·∫°ng yyyymmdd_hhmmss
    V√≠ d·ª•: 20250914_160512
    """
    return datetime.now().strftime("%Y%m%d_%H%M%S")
    
file_path = f"spf_results_{current_timestamp()}.xlsx"
fp = st.empty()
if not os.path.exists(file_path):
    wb = openpyxl.Workbook()
    ws = wb.active
    ws.title = "ShopeeFood"
    # t·∫°o header
    headers = ["url", "crawl_time", "name_restaurant", "status", "brand_title", "brand_url", 
               "danh_hieu", "danh_muc", "chi_nhanh", "date_time", "notification"]
    ws.append(headers)
    wb.save(file_path)
fp = st.write(f"Resutl in {file_path}")
# T√πy ch·ªçn ch·∫°y ·∫©n
chrome_options = Options()
chrome_options.add_argument("--headless")  # b·ªè d√≤ng n√†y n·∫øu mu·ªën nh√¨n th·∫•y Chrome ch·∫°y
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-logging")
chrome_options.add_argument("--log-level=3")
service = Service("E:/cd/chromedriver.exe")  # ƒë∆∞·ªùng d·∫´n ChromeDriver
driver = webdriver.Chrome(service=service, options=chrome_options)
    
def scrape_shopeefood(url, driver):
    try:
        driver.get(url)
        time.sleep(5)  # ho·∫∑c d√πng WebDriverWait ƒë·ªÉ ch√≠nh x√°c

        soup = BeautifulSoup(driver.page_source, "html.parser")

        data = {}

        # Tr·∫°ng th√°i m·ªü c·ª≠a l·∫•y t·ª´ opentime-status
        status_tag = soup.select_one("div.opentime-status span.stt")
        if status_tag:
            data["status"] = status_tag.get("title")  # c√≥ th·ªÉ "M·ªü c·ª≠a" ho·∫∑c "ƒê√£ ƒë√≥ng"
        else:
            data["status"] = None


        # T√™n nh√† h√†ng
        name_restaurant = soup.select_one("h1.name-restaurant")
        data["name_restaurant"] = name_restaurant.get_text(strip=True) if name_restaurant else None

        # kind-restaurant
        kind_restaurant = soup.select_one("div.kind-restaurant")

        # Kh·ªüi t·∫°o m·∫∑c ƒë·ªãnh
        data["danh_hieu"] = None
        data["danh_muc"] = None
        data["chi_nhanh"] = None
        data["notification"] = None
        if kind_restaurant:
            parts = [t.strip() for t in kind_restaurant.stripped_strings if t.strip() not in ["-", "‚Äì"]]

            # Danh hi·ªáu: th∆∞·ªùng n·∫±m trong <div class="tag-preferred">
            tag_preferred = kind_restaurant.select_one("div.tag-preferred")
            if tag_preferred:
                data["danh_hieu"] = tag_preferred.get_text(strip=True)

            # Danh m·ª•c: text ƒë·∫ßu ti√™n ngo√†i tag-preferred
            danh_muc_candidates = [p for p in parts if p != data["danh_hieu"] and "Chi nh√°nh" not in p]
            if danh_muc_candidates:
                data["danh_muc"] = danh_muc_candidates[0]

            # Chi nh√°nh: l·∫•y t·ª´ text c·ªßa a.link-brand
            chi_nhanh_tag = kind_restaurant.select_one("a.link-brand")
            if chi_nhanh_tag:
                data["chi_nhanh"] = chi_nhanh_tag.get_text(strip=True)
        
        # l·∫•y element modal-body
        # t√¨m modal-body
        # t√¨m t·∫•t c·∫£ modal-content
        modals = soup.select("div.modal-content")

        for modal in modals:
            header = modal.select_one("div.txt-bold.font13 span.txt-red")
            if header and header.text.strip() == "ShopeeFood":
                # l·∫•y text modal-body
                modal_body = modal.select_one("div.modal-body")
                if modal_body:
                    text = ". ".join(modal_body.get_text(strip=True, separator="\n").splitlines())
                    print(text)
                    data["notification"] = text

        # Brand: l·∫•y c·∫£ title v√† href
        brand_tag = soup.select_one("a.link-brand")
        if brand_tag:
            data["brand_title"] = brand_tag.get("title")
            data["brand_url"] = brand_tag.get("href")
        else:
            data["brand_title"] = None
            data["brand_url"] = None
        return data
    except Exception as e:
        print(e)

# --- Giao di·ªán Streamlit ---
st.title("ShopeeFood Scraper üçï")
link_input = st.text_area("Nh·∫≠p list link ShopeeFood (m·ªói link 1 d√≤ng):")

col1, col2 = st.columns(2)  # chia 2 c·ªôt

# N√∫t download ·ªü c·ªôt 1
with col2:
    if os.path.exists(file_path):
        with open(file_path, "rb") as f:
            st.download_button(
                label="‚¨áÔ∏è T·∫£i Excel hi·ªán t·∫°i",
                data=f,
                file_name=file_path.split("/")[-1],  # ch·ªâ l·∫•y t√™n file
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )

with col1:   
    crawl_clicked = st.button("DO CRAWL", use_container_width=True) 
if crawl_clicked:
    urls = [u.strip() for u in link_input.split("\n") if u.strip()]
    results = []

    progress_placeholder = st.empty()  # hi·ªÉn th·ªã ti·∫øn ƒë·ªô
    current_item = st.empty()          # link ƒëang crawl
    table_placeholder = st.empty()     # b·∫£ng k·∫øt qu·∫£

    total = len(urls)
    overall_start = time.time()

    for idx, url in enumerate(urls, start=1):
        # c·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
        pro_text = f"‚è≥ Crawling {idx}/{total} ({round(idx/total*100, 1)}%) - {url}"
        progress_placeholder.write(pro_text)
        print(pro_text)
        # current_item.write(f"üîç ƒêang crawl: {url}")

        start = time.time()
        
        try:
            ct = current_timestamp()
            data = scrape_shopeefood(url, driver)
            data["url"] = url
            data["crawl_time"] = round(time.time() - start, 2)
            # l∆∞u 
            row = [
                data.get("url", None),
                data.get("crawl_time", None),
                data.get("name_restaurant", None),
                data.get("status", None),
                data.get("brand_title", None),
                data.get("brand_url", None),
                data.get("danh_hieu", None),
                data.get("danh_muc", None),
                data.get("chi_nhanh", None),
                ct,
                data.get("notification", None)
                
            ]

            wb = openpyxl.load_workbook(file_path)
            ws = wb.active
            ws.append(row)
            wb.save(file_path)

            results.append(data)

            # c·∫≠p nh·∫≠t b·∫£ng ngay sau m·ªói link
            df = pd.DataFrame(results)
            table_placeholder.dataframe(df, use_container_width=True)

        except Exception as e:
            st.error(f"Error {url}: {e}")

    driver.quit()

    if results:
        st.success(f"‚úÖ Ho√†n th√†nh {total}/{total} - T·ªïng th·ªùi gian: {round((time.time()-overall_start)/60,0)}m")

