token = "Bearer $2a$10$47WeoE0UFrhlxluqV5oVReAgjkJLiMQ4PtZLmsHbSv5M5unVgicFq"
import subprocess, io, sys, time, traceback

def install_if_missing(package):
  try:
    __import__(package)
  except ImportError:
    subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--break-system-packages"])
# Cáº§n kiá»ƒm tra vÃ  cÃ i cÃ¡c thÆ° viá»‡n ngoÃ i
for pkg in ["requests", "pandas", "fastapi", "cachetools", "urllib3", "openpyxl", "numpy", "simplejson", "openpyxl", "rapidfuzz", "tqdm"]:
  install_if_missing(pkg)
from typing import List
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
import openpyxl
from rapidfuzz import fuzz, process
import requests
import json
import numpy as np
import pandas as pd
# pd.set_option('future.no_silent_downcasting', True)
from fastapi import FastAPI, Query, Response, Request, HTTPException
from datetime import datetime, timezone, timedelta
from threading import Lock
from fastapi.responses import JSONResponse
import simplejson as json
import logging
import re
import aiohttp
import asyncio
import time, random
from datetime import datetime, timezone, timedelta

app = FastAPI()
cache = {}
# Danh sÃ¡ch skip ban Ä‘áº§u (cÃ³ thá»ƒ lá»›n Ä‘áº¿n 30000)
skips = list(range(0, 30001, 180))

# Biáº¿n cá» dá»«ng toÃ n cá»¥c
stop_flag = asyncio.Event()

raw_rows = []
raw_logs = []
MAX_RETRIES = 10

def get_current_time_str():
  tz_gmt7 = timezone(timedelta(hours=7))
  now = datetime.now(tz_gmt7)
  return now.strftime('%Y-%m-%d_%H-%M-%S')
  
def appendToRow(myDict: dict, key: str, value: str):
  myDict[key] = value
 
def convert_utc_to_gmt7(dt_str: str) -> str:
    """
    Chuyá»ƒn chuá»—i thá»i gian ISO UTC (cÃ³ Ä‘á»‹nh dáº¡ng: 2025-06-26T05:09:58.660403+00:00)
    sang Ä‘á»‹nh dáº¡ng tÆ°Æ¡ng á»©ng trong GMT+7.

    Tráº£ vá» chuá»—i ISO format theo GMT+7.
    """
    try:
        # Parse tá»« chuá»—i cÃ³ timezone UTC
        dt_utc = datetime.strptime(dt_str, "%Y-%m-%dT%H:%M:%S.%f%z")
        # Chuyá»ƒn sang GMT+7
        gmt7 = dt_utc.astimezone(timezone(timedelta(hours=7)))
        return str(gmt7)
    except Exception as e:
        print(e)
        raise ValueError(f"KhÃ´ng thá»ƒ parse thá»i gian: {e}")

def dedup_dicts_smart(data: list[dict]) -> list[dict]:
    try:
        # return pd.DataFrame(data).drop_duplicates().to_dict(orient="records")
        df = pd.DataFrame(data).drop_duplicates()
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df = df.where(pd.notnull(df), None)
        return df
    except Exception as e:
        print(e)
        
async def fetch_worker(worker_id: int, queue: asyncio.Queue, session, stats: dict):
    while not queue.empty() and not stop_flag.is_set():
        url, skipp = await queue.get()
        await fetch_url_with_retry(worker_id, url, session, stats)

async def fetch_url_with_retry(worker_id: int, url: str, session, stats: dict):
    headers = {"Authorization": token}
    retries = 0

    while retries < MAX_RETRIES and not stop_flag.is_set():
        print(f"[Worker-{worker_id}] ðŸš€ Fetching {url} (try {retries + 1}/{MAX_RETRIES})")
        await asyncio.sleep(random.uniform(5, 20))

        try:
            async with session.get(url, headers=headers, ssl=False) as response:
                if response.status == 401:
                    print(f"[Worker-{worker_id}] ðŸ” Token sai táº¡i {url}")
                    stop_flag.set()
                    return

                # Retry Ä‘Æ°á»£c vá»›i lá»—i 5xx
                if 500 <= response.status < 600:
                    print(f"[W-{worker_id}] ðŸ” HTTP {response.status} - thá»­ láº¡i {url}")
                    retries += 1
                    continue

                # KhÃ´ng retry vá»›i lá»—i khÃ¡c (404, 403...)
                if response.status != 200:
                    print(f"[W-{worker_id}] âŒ HTTP {response.status} táº¡i {url} - khÃ´ng retry")
                    break

                data = await response.json()
                sources = data
                if not isinstance(data, list):
                    print(f"[W-{worker_id}] âš ï¸ Dá»¯ liá»‡u khÃ´ng pháº£i list: {data}")
                    break

                if not data:
                    print(f"[Worker-{worker_id}] ðŸ›‘ Dá»«ng láº¡i: {url} tráº£ vá» rá»—ng")
                    stop_flag.set()
                    return

                # Náº¿u thÃ nh cÃ´ng
                count = len(data)
                size = len(json.dumps(data).encode("utf-8"))
                stats["total_items"] += count
                stats["total_bytes"] += size
                excluded_keys = ["weight","area","google_map_address","description","stage_compact","amount", "invoice", "invoices", "opportunity_process",
                               "opportunity_process_stage_id","tax_inclusive_amount","forecast","opportunities_joint","opportunities_products","date_closed",
                               "locked","date_closed_actual","discussions","is_parent","source","opportunity_status","project_type","opportunities_contacts",
                               "Error","notes","parent_opportunity_id","parent_opportunity","opportunities_children","opportunity_type_id","activities","date_open"]
                special_keys = ["custom_field_opportunity_values","opportunity_process_stage","owner","users_opportunities","accounts_opportunities","created_at","stage_logs"]
                for index, item in enumerate(sources):
                    row = {}
                    for key, value in item.items():
                      # print(key)
                      if key not in excluded_keys:
                        if key not in special_keys:
                          appendToRow(row, f'store_{key}',value)
                        elif key == "created_at":
                          appendToRow(row, f'store_{key}',value[:10])
                        elif key == "stage_logs":
                          # print(len(value))
                          for i in value:
                            row_log = {}
                            appendToRow(row_log, f'store_id',item["id"])
                            appendToRow(row_log, f'store_short_id',item["short_id"])
                            appendToRow(row_log, f'creator',i["creator"]["email"])
                            appendToRow(row_log, f'datetime', convert_utc_to_gmt7(i["created_at"]))
                            appendToRow(row_log, f'stage',i["new_stage"])
                            raw_logs.append(row_log)
                        elif key =="custom_field_opportunity_values":
                          for i in value:
                            # print(i["custom_field"])
                            if i["custom_field"]["name"] not in ["20. ADO","23. GiÃ¡ mÃ³n trung bÃ¬nh *","18. VÄ© Ä‘á»™","19. Kinh Ä‘á»™","21. Quáº­n *","Quáº­n (cÅ©)",
                                                                 "24. Pháº§n má»m bÃ¡n hÃ ng *","23. Khung giá» hoáº¡t Ä‘á»™ng 2","25. Ghi ChÃº RiÃªng"]:
                                appendToRow(row, f'store_{i["custom_field"]["name"]}',i["value"])
                        elif key == "opportunity_process_stage":
                          appendToRow(row, f'store_{key}',value["opportunity_stage"]["name"])
                        elif key == "owner":
                          appendToRow(row, f'store_{key}',value["email"])
                        elif key == "users_opportunities":
                          appendToRow(row, f'store_{key}',value[0]["user"]["email"])
                        elif key == "accounts_opportunities":
                          for k, v in value[0]["account"].items():
                            included_keys = ["id","name","short_id","account_type","owner","custom_field_account_values","tax_identification_number"]
                            if k in included_keys:
                              if k == "account_type":
                                appendToRow(row, f'mex_{k}',v)
                              elif k == "tax_identification_number":
                                appendToRow(row, f'mex_tax_id',v)
                              elif k == "owner":
                                appendToRow(row, f'mex_{k}',v["email"])
                              elif k == "custom_field_account_values":
                                for k1 in v:
                                  excluded_keys_mex = ["34. Link áº£nh","21. Pháº§n má»m bÃ¡n hÃ ng *","28. Ghi chÃº tráº¡ng thÃ¡i","27. Tráº¡ng thÃ¡i kÃ½ káº¿t *","29. LÃ½ do KhÃ´ng Há»£p Lá»‡",
                                                       "24. Pháº§n má»m bÃ¡n hÃ ng *","23. Khung giá» hoáº¡t Ä‘á»™ng 2","25. Ghi ChÃº RiÃªng","20. ADO","23. GiÃ¡ mÃ³n trung bÃ¬nh *","24. Link gian hÃ ng SPF",
                                                      "25. Link gian hÃ ng GF", "27. Link gian hÃ ng Google Review","33. Link gian hÃ ng BeF","account_type"]
                                  n = k1["custom_field"]["name"]
                                  vl = k1["value"]
                                  if k1["custom_field"]["master_data_custom_fields"]:
                                    for i in k1["custom_field"]["master_data_custom_fields"]:
                                      if i["id"] == vl:
                                        vl = i["value"]
                                  if n not in excluded_keys_mex:
                                    appendToRow(row, f'mex_{n}',vl)
                              else:
                                appendToRow(row, f'mex_{k}',v)
                        else:
                            appendToRow(row, f'store_{key}',value)
                    raw_rows.append(row)

                print(f"[W-{worker_id}] âœ… {count} item tá»« {url}")
                return  # káº¿t thÃºc thÃ nh cÃ´ng

        except Exception as e:
            print(f"[W-{worker_id}] â— Lá»—i: {e} táº¡i {url}")
            retries += 1

    print(f"[W-{worker_id}] âŒ Bá» qua {url} sau {MAX_RETRIES} láº§n thá»­")

async def fetch_opportunities_queue():
    start_time = time.time()

    # Táº¡o danh sÃ¡ch URL
    urls = [
        (f"https://api-admin.oplacrm.com/api/public/opportunities?take=180&skip={skipp - 30 if skipp > 0 else 0}", skipp)
        for skipp in skips
    ]

    queue = asyncio.Queue()
    for item in urls:
        await queue.put(item)

    stats = {"total_items": 0, "total_bytes": 0}

    async with aiohttp.ClientSession() as session:
        # Táº¡o 3 worker cháº¡y song song
        tasks = [
            fetch_worker(i + 1, queue, session, stats)
            for i in range(5)
        ]
        await asyncio.gather(*tasks)
    store_records = dedup_dicts_smart(raw_rows)
    store_logs = dedup_dicts_smart(raw_logs)
    sto = get_current_time_str()
    msgg = f"   {sta} -> {sto}: {total_bytes / (1024 * 1024):.2f} MB - {len(store_records)} store records + {len(store_logs)} log records"
    print (msgg)
    send_log(msgg,"main")
    with lock:
      if len(store_records) > 0:
        if token not in cache: cache[token] = {}
        cache[token]["stores"] = store_records
        cache[token]["stage_logs"] = store_logs
        cache[token]["updated_stores_and_stage_logs"] = get_current_time_str()

    
    duration = round(time.time() - start_time, 2)
    print(f"âœ… HoÃ n thÃ nh trong {duration} giÃ¢y")
    # print(f"ðŸ“¦ Tá»•ng pháº§n tá»­: {stats['total_items']}")
    print(f"ðŸ“¦ Tá»•ng pháº§n tá»­: {len(dedup_dicts_smart(raw_rows))} stores & {len(dedup_dicts_smart(raw_logs))} logs")
    print(f"ðŸ’¾ Tá»•ng dung lÆ°á»£ng: {stats['total_bytes']} bytes")
    return [store_records, store_logs]
    # return {
        # "total_time_seconds": duration,
        # "total_items": stats["total_items"],
        # "total_bytes": stats["total_bytes"],
        # "stopped_at_skip": None if not stop_flag.is_set() else True
    # }

@app.get("/opla/")
async def api_opla(
    token: str = Query(...),
    secrets: str = Query(...),
    fields: List[str] = Query(None),
    limit: int = Query(None),
    export: int = Query(None)
):
    try:
        if secrets == 'chucm@ym@n8686':
            print(cache.get(token, {}).get("stores") == None)
            if cache.get(token, {}).get("stores") is None:
                await fetch_opportunities_queue()
            df = pd.DataFrame(cache[token]["stores"])
            if limit:
                df = df.iloc[:limit]
            if fields:
                valid_fields = [col for col in fields if col in df.columns]
                df = df[valid_fields]
            if not export or export != 1:
                safe_json = json.dumps(
                    df.to_dict(orient="records"),
                    ignore_nan=True
                )
                return Response(content=safe_json, media_type="application/json")
            elif export == 1:                   
                output = io.BytesIO()
                with pd.ExcelWriter(output, engine="openpyxl") as writer:
                    df.to_excel(writer, index=False, sheet_name=cache[token]["updated_stores_and_stage_logs"])

                output.seek(0)
                file_content = output.read()
                
                file_bytes = io.BytesIO(file_content)
                file_bytes.seek(0)
                # Gá»­i tá»›i Telegram
                send_excel_to_telegram(
                    file_bytes= file_bytes,
                    filename=f"api_store_{cache[token]['updated_stores_and_stage_logs']}.xlsx",
                    chat_id="716085753",
                    bot_token=telegram_token
                )

                # Tráº£ response
                headers = {
                    "Content-Disposition": f"attachment; filename=api_store_{cache[token]['updated_stores_and_stage_logs']}.xlsx"
                }
                return Response(
                    content=file_content,
                    media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    headers=headers
                )
        else:
            return {"Lá»—i": "Sai secrets :("}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Lá»—i: {str(e)}")
